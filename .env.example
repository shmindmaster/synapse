# ============================================
# SYNAPSE ENVIRONMENT CONFIGURATION
# ============================================
# Copy this file to .env and fill in your values:
# cp .env.example .env

# ============================================
# ‚ö†Ô∏è REQUIRED CONFIGURATION
# ============================================

# DATABASE (REQUIRED)
# PostgreSQL connection string with pgvector support
# Format: postgresql://user:password@host:port/database
DATABASE_URL="postgresql://postgres:postgres@localhost:5432/synapse"

# AUTHENTICATION (REQUIRED)
# Secret key for JWT token signing - MUST be changed in production!
# Generate with: openssl rand -base64 32
AUTH_SECRET="change-this-to-a-random-secret-in-production"

# ============================================
# üîí AI PROVIDER: CHOOSE YOUR PATH
# ============================================
# Synapse works with EITHER local models OR cloud APIs.
# Pick one based on your priorities:
#
# LOCAL (Recommended for privacy):
#   ‚úÖ 100% offline, zero API costs
#   ‚úÖ Complete data privacy
#   ‚úÖ HIPAA/GDPR/SOC2 compliant
#   üëâ Jump to section: LOCAL/OFFLINE MODELS
#
# CLOUD (Faster setup, pay-per-use):
#   ‚úÖ No local hardware needed
#   ‚úÖ State-of-art models
#   üëâ Add your key below and skip local sections

# CLOUD PROVIDER API KEY (Optional - only needed if NOT using local models)
# Get your key: https://platform.openai.com/api-keys
# Free tier: $5 credits for new accounts
OPENAI_API_KEY=""

# ============================================
# üîß OPTIONAL MODEL CONFIGURATION
# ============================================
# Default models (change if needed)
MODEL_CHAT="gpt-4o"                           # Primary chat model
MODEL_FAST="gpt-3.5-turbo"                    # Fast operations
MODEL_EMBEDDING="text-embedding-3-small"      # Embeddings (1536 dimensions)

# Alternative AI providers (use instead of OpenAI if preferred)
# GROQ_API_KEY=""                            # Fast, cheap inference
# GEMINI_API_KEY=""                          # Google's models
# ANTHROPIC_API_KEY=""                       # Claude models
# CEREBRAS_API_KEY=""                        # Ultra-fast inference

# ============================================
# ‚òÅÔ∏è AZURE OPENAI CONFIGURATION
# ============================================
# Use Azure OpenAI instead of standard OpenAI API
# See: docs/azure-openai-integration.md for setup guide
#
# Enable Azure OpenAI mode
# USE_AZURE_OPENAI=true
#
# Azure OpenAI Resource Configuration
# AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
# AZURE_OPENAI_API_KEY="your-azure-openai-api-key"
# AZURE_OPENAI_API_VERSION="2024-07-01-preview"
#
# Deployment Names (NOT model names)
# AZURE_OPENAI_CHAT_DEPLOYMENT="gpt-4-deployment"
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT="embedding-deployment"
#
# Optional: Microsoft Entra ID Authentication (recommended for production)
# AZURE_OPENAI_USE_ENTRA=true
# AZURE_CLIENT_ID="your-client-id"
# AZURE_TENANT_ID="your-tenant-id"
# AZURE_CLIENT_SECRET="your-client-secret"

# ============================================
# üíª LOCAL/OFFLINE MODELS CONFIGURATION
# ============================================
# Run completely offline without cloud AI services
# See: docs/local-offline-deployment.md for setup guide
#
# Enable local model mode
# USE_LOCAL_MODELS=true
#
# Chat Model (llama.cpp or vLLM)
# LOCAL_LLM_ENDPOINT="http://localhost:8080"
# LOCAL_LLM_MODEL="Llama-3.2-3B-Instruct"
# LOCAL_LLM_CONTEXT_LENGTH=4096
# LOCAL_LLM_TIMEOUT=30000
#
# Embedding Model (sentence-transformers)
# LOCAL_EMBEDDING_ENDPOINT="http://localhost:8081"
# LOCAL_EMBEDDING_MODEL="all-MiniLM-L6-v2"
# LOCAL_EMBEDDING_DIMENSIONS=384
# EMBEDDING_BATCH_SIZE=10

# ============================================
# üì¶ OPTIONAL OBJECT STORAGE
# ============================================
# Required ONLY if you want to upload files via the web UI
# If not set, files will be stored in the database (works fine for small files)
#
# For DigitalOcean Spaces (S3-compatible):
# SPACES_KEY="your-spaces-access-key"
# SPACES_SECRET="your-spaces-secret-key"
# SPACES_ENDPOINT="https://nyc3.digitaloceanspaces.com"
# SPACES_BUCKET="your-bucket-name"
# OBJECT_STORAGE_PREFIX="synapse/"
#
# For AWS S3:
# OBJECT_STORAGE_KEY="your-aws-access-key"
# OBJECT_STORAGE_SECRET="your-aws-secret-key"
# OBJECT_STORAGE_ENDPOINT="https://s3.amazonaws.com"
# OBJECT_STORAGE_BUCKET="your-bucket-name"
# OBJECT_STORAGE_REGION="us-east-1"

# ============================================
# üåê APPLICATION SETTINGS
# ============================================
NODE_ENV="development"                        # development | production | test
PORT="8000"                                   # Backend server port
APP_SLUG=""                                   # Optional: Multi-tenant identifier

# Frontend URLs (auto-configured in Docker)
VITE_API_URL="http://localhost:8000"
VITE_APP_URL="http://localhost:3000"

# ============================================
# üìä OPTIONAL MONITORING
# ============================================
# Sentry error tracking
# SENTRY_DSN="your-sentry-dsn-here"

# Debug logging
# DEBUG=false

# ============================================
# üöÄ QUICK START GUIDE
# ============================================
#
# PATH 1: LOCAL/OFFLINE (Recommended for privacy)
# ------------------------------------------------
# 1. Copy this file: cp .env.example .env
# 2. Uncomment "USE_LOCAL_MODELS=true" in LOCAL section below
# 3. Follow setup guide: docs/local-offline-deployment.md
# 4. Run: ./quick-start.sh (or quick-start.bat on Windows)
# 5. Open: http://localhost:3000
#
# PATH 2: CLOUD AI (Faster setup)
# --------------------------------
# 1. Copy this file: cp .env.example .env
# 2. Add your OPENAI_API_KEY above (get free $5 credits)
# 3. Run: ./quick-start.sh
# 4. Open: http://localhost:3000
#
# For Cloud Platforms (Railway, DigitalOcean):
# ---------------------------------------------
# See: docs/deployment.md for platform-specific guides
#
# ============================================
# ‚ö†Ô∏è IMPORTANT NOTES
# ============================================
#
# About AI Providers:
# - LOCAL models: No API key needed, 100% offline
# - CLOUD providers: Requires API key, pay-per-use
# - Both work equally well - choose based on privacy vs convenience
#
# About Object Storage:
# - OPTIONAL - only needed for web UI file uploads
# - CLI can index files without object storage
# - Files under 10MB work fine in database storage
#
# About Database:
# - Must have pgvector extension
# - Use PostgreSQL 14+ (16 recommended)
# - Cloud platforms auto-configure this
#
# About Secrets:
# - Never commit .env file to git
# - Use different secrets for dev/prod
# - Rotate keys regularly
# - Use secrets manager in production
