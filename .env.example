# ============================================
# SYNAPSE ENVIRONMENT CONFIGURATION
# ============================================
# Copy this file to .env and fill in your values:
# cp .env.example .env

# ============================================
# ‚ö†Ô∏è REQUIRED CONFIGURATION
# ============================================

# DATABASE (REQUIRED)
# PostgreSQL connection string with pgvector support
# Format: postgresql://user:password@host:port/database
# For Docker: use 'postgres' as host (service name)
# For local dev: use 'localhost'
DATABASE_URL="postgresql://synapse:synapse@postgres:5432/synapse"

# AUTHENTICATION (REQUIRED)
# Secret key for JWT token signing - MUST be changed in production!
# Generate with: openssl rand -base64 32
AUTH_SECRET="change-this-to-a-random-secret-in-production"

# ============================================
# üîí AI PROVIDER: CHOOSE YOUR PATH
# ============================================
# Synapse works with EITHER local models OR cloud APIs.
# Pick one based on your priorities:
#
# LOCAL (Recommended for privacy):
#   ‚úÖ 100% offline, zero API costs
#   ‚úÖ Complete data privacy
#   ‚úÖ HIPAA/GDPR/SOC2 compliant
#   üëâ Jump to section: LOCAL/OFFLINE MODELS
#
# CLOUD (Faster setup, pay-per-use):
#   ‚úÖ No local hardware needed
#   ‚úÖ State-of-art models
#   üëâ Add your key below and skip local sections

# CLOUD PROVIDER API KEY (Optional - only needed if NOT using local models)
# Get your key: https://platform.openai.com/api-keys
# Free tier: $5 credits for new accounts
OPENAI_API_KEY=""

# ============================================
# üîß OPTIONAL MODEL CONFIGURATION
# ============================================
# Default models (change if needed)
MODEL_CHAT="gpt-4o"                           # Primary chat model
MODEL_FAST="gpt-3.5-turbo"                    # Fast operations
MODEL_EMBEDDING="text-embedding-3-small"      # Embeddings (1536 dimensions)

# Alternative AI providers (use instead of OpenAI if preferred)
# GROQ_API_KEY=""                            # Fast, cheap inference
# GEMINI_API_KEY=""                          # Google's models
# ANTHROPIC_API_KEY=""                       # Claude models
# CEREBRAS_API_KEY=""                        # Ultra-fast inference

# ============================================
# ‚òÅÔ∏è AZURE OPENAI CONFIGURATION
# ============================================
# Use Azure OpenAI instead of standard OpenAI API
# Azure provides enterprise features like VNET support and data residency
#
# Enable Azure OpenAI mode
# USE_AZURE_OPENAI=true
#
# Azure OpenAI Resource Configuration
# AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
# AZURE_OPENAI_API_KEY="your-azure-openai-api-key"
# AZURE_OPENAI_API_VERSION="2024-07-01-preview"
#
# Deployment Names (NOT model names)
# AZURE_OPENAI_CHAT_DEPLOYMENT="gpt-4-deployment"
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT="embedding-deployment"
#
# Optional: Microsoft Entra ID Authentication (recommended for production)
# AZURE_OPENAI_USE_ENTRA=true
# AZURE_CLIENT_ID="your-client-id"
# AZURE_TENANT_ID="your-tenant-id"
# AZURE_CLIENT_SECRET="your-client-secret"

# ============================================
# üíª LOCAL/OFFLINE MODELS (Ollama-Powered)
# ============================================
# Run 100% offline with local AI models via Ollama.
# Ollama serves BOTH chat and embeddings from one endpoint.
# No API keys needed. GPU auto-detected if available.
#
# Quick start:
#   1. Uncomment USE_LOCAL_MODELS=true below
#   2. Run: docker compose -f docker-compose.local.yml up -d
#   3. Models download automatically on first run (~4GB)
#
# See: docs/local-offline-deployment.md for full guide
#
# USE_LOCAL_MODELS=true
#
# ---- Chat Model ----
# LOCAL_LLM_ENDPOINT="http://localhost:11434/v1"
# LOCAL_LLM_MODEL="qwen2.5-coder:7b"              # Best for code (default, ~4.5GB)
# LOCAL_LLM_MODEL="gemma3:4b"                      # Lightweight, great on CPU (~2.6GB)
# LOCAL_LLM_MODEL="phi3.5:latest"                  # Microsoft, 128K context (~2.2GB)
# LOCAL_LLM_MODEL="llama3.1:8b"                    # Meta, general-purpose (~4.7GB)
# LOCAL_LLM_CONTEXT_LENGTH=32768
# LOCAL_LLM_TIMEOUT=120000
#
# ---- Embedding Model ----
# LOCAL_EMBEDDING_ENDPOINT="http://localhost:11434/v1"
# LOCAL_EMBEDDING_MODEL="nomic-embed-text"          # 768-dim, best quality (default, ~275MB)
# LOCAL_EMBEDDING_MODEL="all-minilm"                # 384-dim, fastest (~45MB)
# LOCAL_EMBEDDING_DIMENSIONS=768
# EMBEDDING_BATCH_SIZE=50

# ============================================
# üì¶ OPTIONAL OBJECT STORAGE
# ============================================
# Required ONLY if you want to upload files via the web UI
# If not set, files will be stored in the database (works fine for small files)
#
# For DigitalOcean Spaces (S3-compatible):
# SPACES_KEY="your-spaces-access-key"
# SPACES_SECRET="your-spaces-secret-key"
# SPACES_ENDPOINT="https://nyc3.digitaloceanspaces.com"
# SPACES_BUCKET="your-bucket-name"
# OBJECT_STORAGE_PREFIX="synapse/"
#
# For AWS S3:
# OBJECT_STORAGE_KEY="your-aws-access-key"
# OBJECT_STORAGE_SECRET="your-aws-secret-key"
# OBJECT_STORAGE_ENDPOINT="https://s3.amazonaws.com"
# OBJECT_STORAGE_BUCKET="your-bucket-name"
# OBJECT_STORAGE_REGION="us-east-1"

# ============================================
# üåê APPLICATION SETTINGS
# ============================================
NODE_ENV="development"                        # development | production | test
PORT="8000"                                   # Backend server port
APP_SLUG=""                                   # Optional: Multi-tenant identifier

# Frontend URLs (auto-configured in Docker)
VITE_API_URL="http://localhost:8000"
VITE_APP_URL="http://localhost:3000"

# ============================================
# üìä OPTIONAL MONITORING
# ============================================

# Debug logging
# DEBUG=false

# ============================================
# üöÄ QUICK START GUIDE
# ============================================
#
# PATH 1: LOCAL/OFFLINE (Recommended for privacy)
# ------------------------------------------------
# 1. Copy this file: cp .env.example .env
# 2. Uncomment "USE_LOCAL_MODELS=true" in LOCAL section below
# 3. Follow setup guide: docs/local-offline-deployment.md
# 4. Run: ./quick-start.sh (or quick-start.bat on Windows)
# 5. Open: http://localhost:3000
#
# PATH 2: CLOUD AI (Faster setup)
# --------------------------------
# 1. Copy this file: cp .env.example .env
# 2. Add your OPENAI_API_KEY above (get free $5 credits)
# 3. Run: ./quick-start.sh
# 4. Open: http://localhost:3000
#
# For Cloud Platforms (Railway, DigitalOcean):
# ---------------------------------------------
# See: docs/deployment.md for platform-specific guides
#
# ============================================
# ‚ö†Ô∏è IMPORTANT NOTES
# ============================================
#
# About AI Providers:
# - LOCAL models: No API key needed, 100% offline
# - CLOUD providers: Requires API key, pay-per-use
# - Both work equally well - choose based on privacy vs convenience
#
# About Object Storage:
# - OPTIONAL - only needed for web UI file uploads
# - CLI can index files without object storage
# - Files under 10MB work fine in database storage
#
# About Database:
# - Must have pgvector extension
# - Use PostgreSQL 14+ (16 recommended)
# - Cloud platforms auto-configure this
#
# About Secrets:
# - Never commit .env file to git
# - Use different secrets for dev/prod
# - Rotate keys regularly
# - Use secrets manager in production
