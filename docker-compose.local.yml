version: '3.8'

# ============================================
# Synapse Local/Offline Development
# ============================================
# This Docker Compose configuration runs Synapse
# with LOCAL AI models (no cloud AI services).
#
# Prerequisites:
# 1. Download model files to ./models/ directory
# 2. Configure .env with USE_LOCAL_MODELS=true
#
# Usage:
#   docker compose -f docker-compose.local.yml up -d
#
# Documentation: docs/local-offline-deployment.md
# ============================================

services:
  # ============================================
  # PostgreSQL Database with pgvector
  # ============================================
  db:
    image: pgvector/pgvector:pg16
    container_name: synapse-db-local
    environment:
      POSTGRES_USER: synapse
      POSTGRES_PASSWORD: synapse
      POSTGRES_DB: synapse
    volumes:
      - postgres_data_local:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U synapse" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - synapse-local

  # ============================================
  # llama.cpp Server (Local Chat Model)
  # ============================================
  # Download model first:
  #   curl -L -o models/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  #     https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
  llama-cpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: synapse-llama-local
    command: >
      --server --model /models/Llama-3.2-3B-Instruct-Q4_K_M.gguf --host 0.0.0.0 --port 8080 --ctx-size 4096 --n-gpu-layers 0 --threads 8
    volumes:
      - ./models:/models:ro
    ports:
      - "8080:8080"
    environment:
      # Optimize for CPU
      - OMP_NUM_THREADS=8
      - KMP_AFFINITY=granularity=fine,compact,1,0
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 8G
        reservations:
          memory: 4G
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - synapse-local

  # ============================================
  # Sentence Transformers (Local Embeddings)
  # ============================================
  embeddings:
    build:
      context: ./services/embeddings
      dockerfile: Dockerfile
    container_name: synapse-embeddings-local
    ports:
      - "8081:8081"
    environment:
      - MODEL_NAME=all-MiniLM-L6-v2
      - EMBEDDING_DIMENSIONS=384
      - DEVICE=cpu
      - BATCH_SIZE=32
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - synapse-local

  # ============================================
  # Synapse Backend API
  # ============================================
  backend:
    build:
      context: ./apps/backend
      dockerfile: Dockerfile
    container_name: synapse-backend-local
    depends_on:
      db:
        condition: service_healthy
      llama-cpp:
        condition: service_healthy
      embeddings:
        condition: service_healthy
    environment:
      # Database
      - NODE_ENV=production
      - DATABASE_URL=postgresql://synapse:synapse@db:5432/synapse

      # Local Models Configuration
      - USE_LOCAL_MODELS=true
      - LOCAL_LLM_ENDPOINT=http://llama-cpp:8080
      - LOCAL_LLM_MODEL=Llama-3.2-3B-Instruct
      - LOCAL_LLM_CONTEXT_LENGTH=4096
      - LOCAL_LLM_TIMEOUT=30000
      - LOCAL_EMBEDDING_ENDPOINT=http://embeddings:8081
      - LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2
      - LOCAL_EMBEDDING_DIMENSIONS=384
      - EMBEDDING_BATCH_SIZE=10

      # Authentication
      - AUTH_SECRET=${AUTH_SECRET:-change-this-to-a-random-secret-in-production}

      # Server
      - PORT=3000
      - CORS_ORIGIN=http://localhost:5173
    ports:
      - "8000:3000"
    volumes:
      - ./apps/backend:/app
      - /app/node_modules
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:3000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    networks:
      - synapse-local

  # ============================================
  # Synapse Frontend (React + Vite)
  # ============================================
  frontend:
    build:
      context: ./apps/frontend
      dockerfile: Dockerfile
      args:
        - VITE_API_URL=http://localhost:8000
    container_name: synapse-frontend-local
    depends_on:
      - backend
    environment:
      - VITE_API_URL=http://localhost:8000
    ports:
      - "3000:80"
    networks:
      - synapse-local

volumes:
  postgres_data_local:
    driver: local

networks:
  synapse-local:
    driver: bridge

# ============================================
# USAGE INSTRUCTIONS
# ============================================
#
# 1. DOWNLOAD MODEL FILES
#    Create models/ directory and download:
#
#    mkdir -p models
#
#    # Download chat model (~2GB)
#    curl -L -o models/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
#      https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
#
# 2. CREATE EMBEDDING SERVICE
#    Create services/embeddings/Dockerfile and embedding_server.py
#    See: docs/local-offline-deployment.md
#
# 3. CONFIGURE ENVIRONMENT
#    Copy .env.example to .env:
#
#    cp .env.example .env
#
#    Set in .env:
#    USE_LOCAL_MODELS=true
#
# 4. START SERVICES
#    docker compose -f docker-compose.local.yml up -d
#
# 5. CHECK STATUS
#    docker compose -f docker-compose.local.yml ps
#    docker compose -f docker-compose.local.yml logs -f
#
# 6. ACCESS SYNAPSE
#    Open: http://localhost:3000
#    Login: demomaster@pendoah.ai / Pendoah1225
#
# 7. STOP SERVICES
#    docker compose -f docker-compose.local.yml down
#
# ============================================
# TROUBLESHOOTING
# ============================================
#
# "llama-cpp unhealthy":
#   - Verify model file exists in ./models/
#   - Check model path in command matches filename
#   - Increase memory limit if OOM errors
#   - Check logs: docker compose logs llama-cpp
#
# "embeddings unhealthy":
#   - Verify services/embeddings/Dockerfile exists
#   - Check embedding_server.py is present
#   - Install dependencies: pip install flask sentence-transformers
#   - Check logs: docker compose logs embeddings
#
# "backend can't connect":
#   - Ensure llama-cpp and embeddings are healthy
#   - Verify USE_LOCAL_MODELS=true in .env
#   - Check network: docker network inspect synapse-local_synapse-local
#   - Test endpoints:
#     curl http://localhost:8080/health
#     curl http://localhost:8081/health
#
# "slow responses":
#   - Increase CPU allocation: cpus: '12'
#   - Use smaller model: Llama-3.2-3B instead of 8B
#   - Lower quantization: Q4_K_M instead of Q5_K_M
#   - Reduce context: --ctx-size 2048
#
# For more help: docs/local-offline-deployment.md
# ============================================
