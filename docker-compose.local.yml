# ============================================
# Synapse Local/Offline Stack (Ollama-Powered)
# ============================================
# Runs Synapse 100% offline with local AI models.
# Ollama serves BOTH chat LLM and embeddings via
# a single OpenAI-compatible API — no manual model
# downloads or Python services required.
#
# Quick Start:
#   cp .env.example .env
#   docker compose -f docker-compose.local.yml up -d
#   # Wait ~2 min for Ollama to pull models on first run
#   open http://localhost:3000
#
# GPU Support:
#   Works automatically if NVIDIA Container Toolkit
#   is installed. Otherwise falls back to CPU.
#
# Documentation: docs/local-offline-deployment.md
# ============================================

services:
  # ============================================
  # PostgreSQL Database with pgvector
  # ============================================
  db:
    image: pgvector/pgvector:pg16
    container_name: synapse-db-local
    environment:
      POSTGRES_USER: synapse
      POSTGRES_PASSWORD: synapse
      POSTGRES_DB: synapse
    volumes:
      - postgres_data_local:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U synapse" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - synapse-local

  # ============================================
  # Ollama — Unified Local AI Runtime
  # ============================================
  # Serves BOTH the chat LLM and embedding model
  # via OpenAI-compatible API on port 11434.
  #
  # Default models (pulled automatically):
  #   Chat:       qwen2.5-coder:7b  (code-focused, 32K context)
  #   Embeddings: nomic-embed-text   (768-dim, Matryoshka)
  #
  # Alternatives (set via LOCAL_LLM_MODEL env var):
  #   gemma3:4b       — Google, 2.6GB, great on CPU
  #   phi3.5:latest   — Microsoft, 128K context, small
  #   llama3.1:8b     — Meta, general-purpose, 128K ctx
  # ============================================
  ollama:
    image: ollama/ollama:latest
    container_name: synapse-ollama-local
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Adjust based on hardware (0 = auto-detect)
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 4G
          # Uncomment for NVIDIA GPU support (requires nvidia-container-toolkit):
          # devices:
          #   - driver: nvidia
          #     count: all
          #     capabilities: [gpu]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/tags" ]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - synapse-local

  # ============================================
  # Model Initializer (pulls models on first run)
  # ============================================
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
  ollama-init:
    image: curlimages/curl:latest
    container_name: synapse-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: [ "sh", "-c" ]
    command:
      - |
        echo "Pulling embedding model: nomic-embed-text..."
        curl -s http://ollama:11434/api/pull -d '{"name": "nomic-embed-text"}'
        echo ""
        echo "Pulling chat model: ${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}..."
        curl -s http://ollama:11434/api/pull -d '{"name": "${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}"}'
        echo ""
        echo "Models ready!"
=======
  embeddings:
    build:
      context: ./src/services/embeddings
      dockerfile: Dockerfile
    container_name: synapse-embeddings-local
    ports:
      - "8081:8081"
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0a1ab70e/docker-compose.local.yml
    environment:
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
    networks:
      - synapse-local

  # ============================================
  # Synapse Backend API
  # ============================================
  backend:
    build:
      context: .
      dockerfile: src/api/Dockerfile
    container_name: synapse-backend-local
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    environment:
      # Database
      - NODE_ENV=production
      - DATABASE_URL=postgresql://synapse:synapse@db:5432/synapse

      # Local Models — Ollama serves both LLM and embeddings
      - USE_LOCAL_MODELS=true
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
      - LOCAL_LLM_ENDPOINT=http://ollama:11434/v1
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
      - LOCAL_LLM_CONTEXT_LENGTH=32768
      - LOCAL_LLM_TIMEOUT=120000
      - LOCAL_EMBEDDING_ENDPOINT=http://ollama:11434/v1
      - LOCAL_EMBEDDING_MODEL=${LOCAL_EMBEDDING_MODEL:-nomic-embed-text}
      - LOCAL_EMBEDDING_DIMENSIONS=768
      - EMBEDDING_DIMENSIONS=768
      - EMBEDDING_BATCH_SIZE=50
=======
      - LOCAL_LLM_ENDPOINT=http://llama-cpp:8080
      - LOCAL_LLM_MODEL=Llama-3.2-3B-Instruct
      - LOCAL_LLM_CONTEXT_LENGTH=4096
      - LOCAL_LLM_TIMEOUT=30000
      - LOCAL_EMBEDDING_ENDPOINT=http://embeddings:8081
      - LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2
      - LOCAL_EMBEDDING_DIMENSIONS=384
      - EMBEDDING_DIMENSIONS=384
      - EMBEDDING_BATCH_SIZE=10
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0a1ab70e/docker-compose.local.yml

      # Authentication
      - AUTH_SECRET=${AUTH_SECRET:-change-this-to-a-random-secret-in-production}

<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
      # Server
=======
      # Server - use same port internally and externally
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0a1ab70e/docker-compose.local.yml
      - PORT=8000
      - CORS_ORIGIN=http://localhost:3000
    ports:
      - "8000:8000"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    networks:
      - synapse-local

  # ============================================
  # Synapse Frontend (React + Vite)
  # ============================================
  frontend:
    build:
      context: .
      dockerfile: src/web/Dockerfile
      args:
        VITE_API_URL: http://localhost:8000
    container_name: synapse-frontend-local
    depends_on:
      - backend
    ports:
      - "3000:3000"
    networks:
      - synapse-local

volumes:
  postgres_data_local:
    driver: local
  ollama_data:
    driver: local

networks:
  synapse-local:
    driver: bridge

# ============================================
# QUICK START
# ============================================
#
# 1. CONFIGURE
#    cp .env.example .env
#    # (defaults work — no edits needed for basic test)
#
# 2. START
#    docker compose -f docker-compose.local.yml up -d
#    # First run: Ollama downloads models (~4GB total, 3-10 min)
#    # Subsequent runs: instant start from cache
#
# 3. MONITOR MODEL DOWNLOADS
#    docker compose -f docker-compose.local.yml logs -f ollama-init
#
# 4. ACCESS
#    http://localhost:3000
#    Login: demo@synapse.local / DemoPassword123!
#
# 5. STOP
#    docker compose -f docker-compose.local.yml down
#    # Models are cached in ollama_data volume (persistent)
#
# ============================================
# MODEL OPTIONS (set in .env)
# ============================================
#
# Chat Models (LOCAL_LLM_MODEL):
#   qwen2.5-coder:7b  — Best for code (default, ~4.5GB)
#   gemma3:4b          — Lightweight, great on CPU (~2.6GB)
#   phi3.5:latest      — Microsoft, 128K context (~2.2GB)
#   llama3.1:8b        — Meta, general-purpose (~4.7GB)
#
# Embedding Models (LOCAL_EMBEDDING_MODEL):
#   nomic-embed-text   — 768-dim, best quality (default, ~275MB)
#   all-minilm          — 384-dim, fastest (~45MB)
#
# ============================================
# GPU SUPPORT
# ============================================
#
# Ollama auto-detects NVIDIA GPUs if nvidia-container-toolkit
# is installed. To enable:
#
# 1. Install: https://docs.nvidia.com/datacenter/cloud-native/
#             container-toolkit/install-guide.html
#
# 2. Uncomment the 'devices' section under ollama.deploy.resources
#
# 3. Restart: docker compose -f docker-compose.local.yml up -d
#
# CPU-only is fully supported — just slower for chat responses.
# Embeddings are fast on CPU regardless.
#
# ============================================
# TROUBLESHOOTING
# ============================================
#
# "ollama unhealthy":
#   - Check logs: docker compose -f docker-compose.local.yml logs ollama
#   - Increase memory limit if OOM
#   - Verify port 11434 is free: lsof -i :11434
#
# "model download stuck":
#   - Check init logs: docker compose logs ollama-init
#   - Manual pull: docker exec synapse-ollama-local ollama pull qwen2.5-coder:7b
#   - Check disk space (models need ~5GB)
#
# "slow responses":
#   - Enable GPU (see GPU SUPPORT above)
#   - Switch to smaller model: LOCAL_LLM_MODEL=gemma3:4b
#   - Reduce context in config: LOCAL_LLM_CONTEXT_LENGTH=4096
#
# "backend can't connect to ollama":
#   - Ensure ollama is healthy: curl http://localhost:11434/api/tags
#   - Check network: docker network inspect synapse-local
#   - Verify LOCAL_LLM_ENDPOINT=http://ollama:11434/v1
#
# For more help: docs/local-offline-deployment.md
# ============================================
