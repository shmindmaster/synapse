# ============================================
# Synapse Local/Offline Stack (Ollama-Powered)
# ============================================
# Runs Synapse 100% offline with local AI models.
# Ollama serves BOTH chat LLM and embeddings via
# a single OpenAI-compatible API.
#
# Quick Start:
#   cp .env.example .env
#   docker compose -f docker-compose.local.yml up -d
#   # Wait ~2 min for Ollama to pull models on first run
#   open http://localhost:3000
#
# GPU Support:
#   Works automatically if NVIDIA Container Toolkit
#   is installed. Otherwise falls back to CPU.
#
# Models download automatically on first run (~5GB)
# Documentation: docs/local-offline-deployment.md
# ============================================

services:
  # ============================================
  # PostgreSQL Database with pgvector
  # ============================================
  postgres:
    image: pgvector/pgvector:pg16
    container_name: synapse-postgres-local
    environment:
      POSTGRES_USER: synapse
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-synapse}
      POSTGRES_DB: synapse
    volumes:
      - postgres_data_local:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U synapse" ]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      postgres -c shared_preload_libraries=vector -c max_connections=200
    networks:
      - synapse-local

  # ============================================
  # Ollama - Unified Local AI Runtime
  # ============================================
  # Serves BOTH the chat LLM and embedding model
  # via OpenAI-compatible API on port 11434.
  #
  # Default models (pulled automatically):
  #   Chat:       qwen2.5-coder:7b  (code-focused, 32K context)
  #   Embeddings: nomic-embed-text   (768-dim, Matryoshka)
  ollama:
    image: ollama/ollama:latest
    container_name: synapse-ollama-local
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 4G
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/tags" ]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
    networks:
      - synapse-local

  # ============================================
  # Model Initializer (pulls models on first run)
  # ============================================
  init-models:
    image: curlimages/curl:latest
    container_name: synapse-init-models
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: [ "sh", "-c" ]
    command:
      - |
        echo "Pulling embedding model: nomic-embed-text..."
        curl -s http://ollama:11434/api/pull -d '{"name": "nomic-embed-text"}'
        echo ""
        echo "Pulling chat model: qwen2.5-coder:7b..."
        curl -s http://ollama:11434/api/pull -d '{"name": "qwen2.5-coder:7b"}'
        echo ""
        echo "Models ready!"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
      - LOCAL_EMBEDDING_MODEL=${LOCAL_EMBEDDING_MODEL:-nomic-embed-text}
    networks:
      - synapse-local
    restart: "no"

  # ============================================
  # Synapse Backend API
  # ============================================
  backend:
    build:
      context: .
      dockerfile: src/api/Dockerfile
    container_name: synapse-backend-local
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_healthy
      init-models:
        condition: service_completed_successfully
    environment:
      # Database
      - NODE_ENV=${NODE_ENV:-production}
      - DATABASE_URL=postgresql://synapse:${POSTGRES_PASSWORD:-synapse}@postgres:5432/synapse

      # Local Models - Ollama serves both LLM and embeddings
      - USE_LOCAL_MODELS=true
      - LOCAL_LLM_ENDPOINT=http://ollama:11434/v1
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
      - LOCAL_LLM_CONTEXT_LENGTH=${LOCAL_LLM_CONTEXT_LENGTH:-32768}
      - LOCAL_LLM_TIMEOUT=${LOCAL_LLM_TIMEOUT:-120000}
      - LOCAL_EMBEDDING_ENDPOINT=http://ollama:11434/v1
      - LOCAL_EMBEDDING_MODEL=${LOCAL_EMBEDDING_MODEL:-nomic-embed-text}
      - LOCAL_EMBEDDING_DIMENSIONS=${LOCAL_EMBEDDING_DIMENSIONS:-768}
      - EMBEDDING_DIMENSIONS=${LOCAL_EMBEDDING_DIMENSIONS:-768}
      - EMBEDDING_BATCH_SIZE=${EMBEDDING_BATCH_SIZE:-50}

      # Authentication
      - AUTH_SECRET=${AUTH_SECRET:-change-this-to-a-random-secret-in-production}

      # Server
      - PORT=8000
      - CORS_ORIGIN=${CORS_ORIGIN:-http://localhost:3000}
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - synapse-local

  # ============================================
  # Synapse Frontend (React + Vite)
  # ============================================
  frontend:
    build:
      context: .
      dockerfile: src/web/Dockerfile
      args:
        VITE_API_URL: ${VITE_API_URL:-http://localhost:8000}
    container_name: synapse-frontend-local
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    networks:
      - synapse-local

volumes:
  postgres_data_local:
    driver: local
  ollama_data:
    driver: local

networks:
  synapse-local:
    driver: bridge

# ============================================
# QUICK START
# ============================================
#
# 1. CONFIGURE
#    cp .env.example .env
#    # (defaults work — no edits needed for basic test)
#
# 2. START
#    docker compose -f docker-compose.local.yml up -d
#    # First run: Ollama downloads models (~4GB total, 3-10 min)
#    # Subsequent runs: instant start from cache
#
# 3. MONITOR MODEL DOWNLOADS
#    docker compose -f docker-compose.local.yml logs -f init-models
#
# 4. ACCESS
#    http://localhost:3000
#    Login: demo@synapse.local / DemoPassword123!
#
# 5. STOP
#    docker compose -f docker-compose.local.yml down
#    # Models are cached in ollama_data volume (persistent)
#
# ============================================
# MODEL OPTIONS (set in .env)
# ============================================
#
# Chat Models (LOCAL_LLM_MODEL):
#   qwen2.5-coder:7b  — Best for code (default, ~4.5GB)
#   gemma3:4b          — Lightweight, great on CPU (~2.6GB)
#   phi3.5:latest      — Microsoft, 128K context (~2.2GB)
#   llama3.1:8b        — Meta, general-purpose (~4.7GB)
#
# Embedding Models (LOCAL_EMBEDDING_MODEL):
#   nomic-embed-text   — 768-dim, best quality (default, ~275MB)
#   all-minilm          — 384-dim, fastest (~45MB)
#
# ============================================
# TROUBLESHOOTING
# ============================================
#
# "ollama unhealthy":
#   - Check logs: docker compose -f docker-compose.local.yml logs ollama
#   - Increase memory limit if OOM
#   - Verify port 11434 is free
#
# "model download stuck":
#   - Check init logs: docker compose logs init-models
#   - Manual pull: docker exec synapse-ollama-local ollama pull qwen2.5-coder:7b
#   - Check disk space (models need ~5GB)
#
# "slow responses":
#   - Enable GPU support (uncomment deploy section in ollama service)
#   - Switch to smaller model: LOCAL_LLM_MODEL=gemma3:4b
#   - Reduce context: LOCAL_LLM_CONTEXT_LENGTH=4096
#
# For more help: docs/local-offline-deployment.md
# ============================================
