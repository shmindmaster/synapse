# ============================================
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
# Synapse Local/Offline Stack (Ollama-Powered)
# ============================================
# Runs Synapse 100% offline with local AI models.
# Ollama serves BOTH chat LLM and embeddings via
# a single OpenAI-compatible API — no manual model
# downloads or Python services required.
=======
# Synapse Local/Offline Development (Ollama)
# ============================================
# This Docker Compose configuration runs Synapse
# with LOCAL AI models via Ollama (no cloud AI).
#
# Prerequisites:
# 1. Docker Desktop with 8GB+ RAM allocated
# 2. (Optional) NVIDIA GPU + nvidia-docker for GPU acceleration
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml
#
# Quick Start:
#   cp .env.example .env
#   docker compose -f docker-compose.local.yml up -d
#   # Wait ~2 min for Ollama to pull models on first run
#   open http://localhost:3000
#
# GPU Support:
#   Works automatically if NVIDIA Container Toolkit
#   is installed. Otherwise falls back to CPU.
#
# Models download automatically on first run (~5GB)
# Documentation: docs/local-offline-deployment.md
# ============================================

services:
  # ============================================
  # PostgreSQL Database with pgvector
  # ============================================
  postgres:
    image: pgvector/pgvector:pg16
    container_name: synapse-postgres-local
    environment:
      POSTGRES_USER: synapse
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-synapse}
      POSTGRES_DB: synapse
    volumes:
      - postgres_data_local:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U synapse"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      postgres -c shared_preload_libraries=vector -c max_connections=200
    networks:
      - synapse-local

  # ============================================
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
  # Ollama — Unified Local AI Runtime
  # ============================================
  # Serves BOTH the chat LLM and embedding model
  # via OpenAI-compatible API on port 11434.
  #
  # Default models (pulled automatically):
  #   Chat:       qwen2.5-coder:7b  (code-focused, 32K context)
  #   Embeddings: nomic-embed-text   (768-dim, Matryoshka)
  #
  # Alternatives (set via LOCAL_LLM_MODEL env var):
  #   gemma3:4b       — Google, 2.6GB, great on CPU
  #   phi3.5:latest   — Microsoft, 128K context, small
  #   llama3.1:8b     — Meta, general-purpose, 128K ctx
  # ============================================
  ollama:
    image: ollama/ollama:latest
    container_name: synapse-ollama-local
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Adjust based on hardware (0 = auto-detect)
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 4G
          # Uncomment for NVIDIA GPU support (requires nvidia-container-toolkit):
          # devices:
          #   - driver: nvidia
          #     count: all
          #     capabilities: [gpu]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/tags" ]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s
=======
  # Ollama (Local LLM + Embeddings)
  # ============================================
  # Serves both chat and embedding models via OpenAI-compatible API
  # GPU acceleration: Uncomment deploy section below if you have NVIDIA GPU
  ollama:
    image: ollama/ollama:latest
    container_name: synapse-ollama-local
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml
    networks:
      - synapse-local
    # Uncomment for GPU support (requires nvidia-docker):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # ============================================
  # Model Initializer (pulls models on first run)
  # ============================================
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
  ollama-init:
    image: curlimages/curl:latest
    container_name: synapse-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: [ "sh", "-c" ]
    command:
      - |
        echo "Pulling embedding model: nomic-embed-text..."
        curl -s http://ollama:11434/api/pull -d '{"name": "nomic-embed-text"}'
        echo ""
        echo "Pulling chat model: qwen2.5-coder:7b..."
        curl -s http://ollama:11434/api/pull -d '{"name": "qwen2.5-coder:7b"}'
        echo ""
        echo "Models ready!"
    environment:
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
=======
  init-models:
    image: curlimages/curl:latest
    container_name: synapse-init-models
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./scripts:/scripts:ro
    entrypoint: ["/bin/sh", "/scripts/init-models.sh"]
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
      - LOCAL_EMBEDDING_MODEL=${LOCAL_EMBEDDING_MODEL:-nomic-embed-text}
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml
    networks:
      - synapse-local
    restart: "no"

  # ============================================
  # Synapse Backend API
  # ============================================
  backend:
    build:
      context: .
      dockerfile: src/api/Dockerfile
    container_name: synapse-backend-local
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_healthy
      init-models:
        condition: service_completed_successfully
    environment:
      # Database
      - NODE_ENV=${NODE_ENV:-production}
      - DATABASE_URL=postgresql://synapse:${POSTGRES_PASSWORD:-synapse}@postgres:5432/synapse

<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
      # Local Models — Ollama serves both LLM and embeddings
      - USE_LOCAL_MODELS=true
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
      - LOCAL_LLM_ENDPOINT=http://ollama:11434/v1
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
      - LOCAL_LLM_CONTEXT_LENGTH=32768
      - LOCAL_LLM_TIMEOUT=120000
      - LOCAL_EMBEDDING_ENDPOINT=http://ollama:11434/v1
      - LOCAL_EMBEDDING_MODEL=${LOCAL_EMBEDDING_MODEL:-nomic-embed-text}
      - LOCAL_EMBEDDING_DIMENSIONS=768
      - EMBEDDING_DIMENSIONS=768
      - EMBEDDING_BATCH_SIZE=50
=======
      - LOCAL_LLM_ENDPOINT=http://llama-cpp:8080
      - LOCAL_LLM_MODEL=Llama-3.2-3B-Instruct
      - LOCAL_LLM_CONTEXT_LENGTH=4096
      - LOCAL_LLM_TIMEOUT=30000
      - LOCAL_EMBEDDING_ENDPOINT=http://embeddings:8081
      - LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2
      - LOCAL_EMBEDDING_DIMENSIONS=384
      - EMBEDDING_DIMENSIONS=384
      - EMBEDDING_BATCH_SIZE=10
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0a1ab70e/docker-compose.local.yml
=======
      # Local Models Configuration (Ollama)
      - USE_LOCAL_MODELS=true
      - LOCAL_LLM_ENDPOINT=http://ollama:11434/v1
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-qwen2.5-coder:7b}
      - LOCAL_LLM_CONTEXT_LENGTH=${LOCAL_LLM_CONTEXT_LENGTH:-32768}
      - LOCAL_LLM_TIMEOUT=${LOCAL_LLM_TIMEOUT:-120000}
      - LOCAL_EMBEDDING_ENDPOINT=http://ollama:11434/v1
      - LOCAL_EMBEDDING_MODEL=${LOCAL_EMBEDDING_MODEL:-nomic-embed-text}
      - LOCAL_EMBEDDING_DIMENSIONS=${LOCAL_EMBEDDING_DIMENSIONS:-768}
      - EMBEDDING_DIMENSIONS=${LOCAL_EMBEDDING_DIMENSIONS:-768}
      - EMBEDDING_BATCH_SIZE=${EMBEDDING_BATCH_SIZE:-50}
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml

      # Authentication
      - AUTH_SECRET=${AUTH_SECRET:-change-this-to-a-random-secret-in-production}

<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
      # Server
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
=======
      # Server - use same port internally and externally
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0a1ab70e/docker-compose.local.yml
      - PORT=8000
      - CORS_ORIGIN=http://localhost:3000
    ports:
      - "8000:8000"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/health" ]
=======
      - PORT=8000
      - CORS_ORIGIN=${CORS_ORIGIN:-http://localhost:3000}
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - synapse-local

  # ============================================
  # Synapse Frontend (React + Vite)
  # ============================================
  frontend:
    build:
      context: .
      dockerfile: src/web/Dockerfile
      args:
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
        VITE_API_URL: http://localhost:8000
    container_name: synapse-frontend-local
    depends_on:
      - backend
    ports:
      - "3000:3000"
=======
        VITE_API_URL: ${VITE_API_URL:-http://localhost:8000}
    container_name: synapse-frontend-local
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml
    networks:
      - synapse-local

volumes:
  postgres_data_local:
    driver: local
  ollama_data:
    driver: local

networks:
  synapse-local:
    driver: bridge

# ============================================
# QUICK START
# ============================================
#
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
# 1. CONFIGURE
#    cp .env.example .env
#    # (defaults work — no edits needed for basic test)
#
# 2. START
#    docker compose -f docker-compose.local.yml up -d
#    # First run: Ollama downloads models (~4GB total, 3-10 min)
#    # Subsequent runs: instant start from cache
#
# 3. MONITOR MODEL DOWNLOADS
#    docker compose -f docker-compose.local.yml logs -f ollama-init
#
# 4. ACCESS
#    http://localhost:3000
#    Login: demo@synapse.local / DemoPassword123!
#
# 5. STOP
#    docker compose -f docker-compose.local.yml down
#    # Models are cached in ollama_data volume (persistent)
#
# ============================================
# MODEL OPTIONS (set in .env)
# ============================================
#
# Chat Models (LOCAL_LLM_MODEL):
#   qwen2.5-coder:7b  — Best for code (default, ~4.5GB)
#   gemma3:4b          — Lightweight, great on CPU (~2.6GB)
#   phi3.5:latest      — Microsoft, 128K context (~2.2GB)
#   llama3.1:8b        — Meta, general-purpose (~4.7GB)
#
# Embedding Models (LOCAL_EMBEDDING_MODEL):
#   nomic-embed-text   — 768-dim, best quality (default, ~275MB)
#   all-minilm          — 384-dim, fastest (~45MB)
#
# ============================================
# GPU SUPPORT
# ============================================
#
# Ollama auto-detects NVIDIA GPUs if nvidia-container-toolkit
# is installed. To enable:
#
# 1. Install: https://docs.nvidia.com/datacenter/cloud-native/
#             container-toolkit/install-guide.html
#
# 2. Uncomment the 'devices' section under ollama.deploy.resources
#
# 3. Restart: docker compose -f docker-compose.local.yml up -d
#
# CPU-only is fully supported — just slower for chat responses.
# Embeddings are fast on CPU regardless.
=======
# 1. CONFIGURE ENVIRONMENT (Optional)
#    cp .env.example .env
#    # Edit .env to customize model choices (optional)
#
# 2. START SERVICES
#    docker compose -f docker-compose.local.yml up -d
#    # Models download automatically (~5GB, 5-10 minutes first run)
#
# 3. CHECK STATUS
#    docker compose -f docker-compose.local.yml ps
#    docker compose -f docker-compose.local.yml logs -f ollama
#
# 4. ACCESS SYNAPSE
#    Open: http://localhost:3000
#    Login: demo@synapse.local / DemoPassword123!
#
# 5. STOP SERVICES
#    docker compose -f docker-compose.local.yml down
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml
#
# ============================================
# CUSTOMIZATION
# ============================================
#
# Change models in .env:
#   LOCAL_LLM_MODEL=gemma3:4b              # Smaller, faster (2.6GB)
#   LOCAL_LLM_MODEL=phi3.5:latest          # Microsoft, 128K context
#   LOCAL_LLM_MODEL=llama3.1:8b            # Meta, general-purpose
#   LOCAL_EMBEDDING_MODEL=all-minilm       # Faster embeddings (384-dim)
#
# Enable GPU (requires nvidia-docker):
#   Uncomment the deploy section in ollama service above
#
# ============================================
# TROUBLESHOOTING
# ============================================
#
<<<<<<< H:/Repos/shmindmaster/synapse/docker-compose.local.yml
# "ollama unhealthy":
#   - Check logs: docker compose -f docker-compose.local.yml logs ollama
#   - Increase memory limit if OOM
#   - Verify port 11434 is free: lsof -i :11434
#
# "model download stuck":
#   - Check init logs: docker compose logs ollama-init
#   - Manual pull: docker exec synapse-ollama-local ollama pull qwen2.5-coder:7b
#   - Check disk space (models need ~5GB)
#
# "slow responses":
#   - Enable GPU (see GPU SUPPORT above)
#   - Switch to smaller model: LOCAL_LLM_MODEL=gemma3:4b
#   - Reduce context in config: LOCAL_LLM_CONTEXT_LENGTH=4096
#
# "backend can't connect to ollama":
#   - Ensure ollama is healthy: curl http://localhost:11434/api/tags
#   - Check network: docker network inspect synapse-local
#   - Verify LOCAL_LLM_ENDPOINT=http://ollama:11434/v1
=======
# "init-models failed":
#   - Check Ollama is healthy: docker compose logs ollama
#   - Verify internet connection (models download from ollama.ai)
#   - Retry: docker compose up init-models --force-recreate
#
# "backend can't connect to ollama":
#   - Check ollama health: curl http://localhost:11434/api/tags
#   - Verify network: docker network inspect synapse-local
#   - Check logs: docker compose logs backend
#
# "slow responses":
#   - Use smaller model: LOCAL_LLM_MODEL=gemma3:4b
#   - Enable GPU (see customization above)
#   - Increase Docker Desktop memory allocation (8GB+ recommended)
#
# "out of memory":
#   - Use smaller model: gemma3:4b or phi3.5
#   - Increase Docker Desktop memory limit
#   - Close other applications
>>>>>>> C:/Users/SaroshHussain/.windsurf/worktrees/synapse/synapse-0ad03b3e/docker-compose.local.yml
#
# For more help: docs/local-offline-deployment.md
# ============================================
